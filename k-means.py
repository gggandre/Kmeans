#----------------------------------------------------------
# Feedback Moment: Module 2
# Implementation of a machine learning technique without the use of a framework.
#
# Date: 27-Aug-2023
# Author:
#           A01753176 Gilberto André García Gaytán
#
# KMeans is a clustering algorithm, not a supervised classification algorithm. Therefore, typical metrics for supervised 
# classification like precision, recall, F1 score, and confusion matrix are not directly applicable.
# Instead, to evaluate the clusters generated by the KMeans algorithm, the following metrics are used:
# 1. Inertia (WCSS - Within-Cluster Sum of Squares): Measures how cohesive the clusters are. It's the sum of the squared distances of each point to its centroid. The aim is to minimize this metric.
# 2. Silhouette Score: Measures how similar an object is to its own cluster compared to other clusters. The value ranges between -1 and 1, where a high value indicates that the object is well matched to its own cluster and poorly matched with neighboring clusters.
# 3. Calinski-Harabasz Index: It's the ratio of dispersion within the cluster and dispersion between clusters. Higher values indicate better clusters.
# 4. Davies-Bouldin Index: This metric indicates the average similarity of each cluster with the most similar cluster. Lower values indicate better clusters.
# These metrics were calculated for both the training and validation datasets to ensure the model generalizes well.
#----------------------------------------------------------

import pandas as pd
import numpy as np
import tkinter as tk
from tkinter import ttk, filedialog, messagebox
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score


def euclidean_distance(point1, point2):
    """
    The function calculates the Euclidean distance between two points in a multi-dimensional space.
    :param point1: The first point in the Euclidean distance calculation
    :param point2: The second point in the Euclidean distance calculation
    :return: the Euclidean distance between point1 and point2.
    """
    return np.sqrt(np.sum((point1 - point2) ** 2))

# The KMeans class implements the K-means clustering algorithm for clustering data into k clusters.
class KMeans:
    def __init__(self, k=3, max_iters=100, random_seed=None):
        """
        The function initializes the parameters for a k-means clustering algorithm.
        :param k: The number of clusters to create. It determines how many centroids will be initialized and
        how many clusters the algorithm will try to find, defaults to 3 (optional)
        :param max_iters: The `max_iters` parameter specifies the maximum number of iterations that the
        algorithm will perform before stopping. This is used to control the convergence of the algorithm,
        defaults to 100 (optional)
        :param random_seed: The `random_seed` parameter is used to set the seed for the random number
        generator. This allows you to reproduce the same results when running the code multiple times. By
        setting a specific value for `random_seed`, you can ensure that the random initialization of
        centroids in the k-means algorithm remains the
        """
        self.k = k
        self.max_iters = max_iters
        self.random_seed = random_seed
        self.centroids = None

    def fit(self, data):
        """
        The `fit` function is used to train a k-means clustering model on the given data.
        :param data: The `data` parameter is a numpy array that represents the dataset on which the K-means
        algorithm will be applied. Each row of the array represents a data point, and each column represents
        a feature of that data point
        :return: The `fit` method returns the instance of the class itself (`self`).
        """
        np.random.seed(self.random_seed)
        random_indices = np.random.choice(len(data), self.k, replace=False)
        self.centroids = data[random_indices, :]
        for _ in range(self.max_iters):
            labels = np.array([self._closest_centroid(point) for point in data])
            new_centroids = []
            for i in range(self.k):
                if (labels == i).any():
                    new_centroids.append(data[labels == i].mean(axis=0))
                else:
                    new_centroids.append(data[np.random.choice(len(data))])
            new_centroids = np.array(new_centroids)
            if np.all(self.centroids == new_centroids):
                break
            self.centroids = new_centroids
        return self

    def predict(self, data):
        """
        The function predicts the closest centroid for each data point.
        :param data: The "data" parameter is a list or array of data points that you want to make
        predictions for. Each data point should be a list or array of features
        :return: a numpy array containing the closest centroid for each point in the input data.
        """
        return np.array([self._closest_centroid(point) for point in data])

    def _closest_centroid(self, point):
        """
        The function `_closest_centroid` calculates the closest centroid to a given point using the
        Euclidean distance.
        :param point: A single data point for which we want to find the closest centroid
        :return: the index of the centroid that is closest to the given point.
        """
        distances = [euclidean_distance(point, centroid) for centroid in self.centroids]
        return np.argmin(distances)

class KMeansGUI(tk.Tk):
    def __init__(self, data, features):
        """
        The above code defines a class with an initialization method that sets up a GUI for running KMeans
        clustering on Spotify data.
        :param data: The "data" parameter is the dataset that you want to perform KMeans clustering on. It
        should be a matrix or a dataframe where each row represents a data point and each column represents
        a feature of that data point
        :param features: The "features" parameter is a list that contains the names of the features or
        attributes of the data that will be used for clustering. These features could be any measurable
        characteristics of the songs in the Spotify data, such as danceability, energy, loudness, etc
        """
        super().__init__()
        self.data = data
        self.features = features
        self.kmeans_model = None
        self.title("KMeans Clustering - Spotify Data")
        tk.Label(self, text="Number of Clusters (k):").pack(pady=10)
        self.k_entry = tk.Entry(self)
        self.k_entry.pack(pady=10)
        self.run_button = tk.Button(self, text="Run KMeans", command=self.run_kmeans)
        self.run_button.pack(pady=20)
        self.tree = ttk.Treeview(self, columns=('Song', 'Cluster'))
        self.tree.heading('#0', text='Index')
        self.tree.heading('Song', text='Song')
        self.tree.heading('Cluster', text='Cluster')
        self.tree.pack(pady=20, padx=20, expand=True, fill='both')

    def run_kmeans(self):
        """
        The function performs k-means clustering on a set of features and displays the resulting clusters in
        a treeview widget.
        :return: The function does not explicitly return anything.
        """
        k = int(self.k_entry.get())
        if k >= len(self.features):
            error_msg = "Entered k value exceeds the number of data points. Please try with a smaller value."
            tk.messagebox.showerror("Error", error_msg)
            return
        self.kmeans_model = KMeans(k=k, random_seed=42)
        self.kmeans_model.fit(self.features)
        clusters = self.kmeans_model.predict(self.features)
        for i in self.tree.get_children():
            self.tree.delete(i)
        for idx, (song_name, cluster) in enumerate(zip(self.data['track_name'], clusters)):
            self.tree.insert('', 'end', text=str(idx+1), values=(song_name, cluster))

def calculate_inertia(data, model):
    """
    The function calculates the inertia of a clustering model given a dataset.
    :param data: The data parameter is a numpy array or a pandas DataFrame containing the data points
    that you want to calculate the inertia for. Each row of the data represents a data point
    :param model: The `model` parameter refers to a clustering model that has been trained on some data.
    It could be any clustering algorithm such as K-means, DBSCAN, or hierarchical clustering
    :return: the inertia value, which is a measure of how internally coherent the clusters are.
    """
    centroids = model.centroids
    clusters = model.predict(data)
    inertia = 0
    for i in range(len(data)):
        centroid = centroids[clusters[i]]
        inertia += np.sum((data[i] - centroid) ** 2)
    return inertia

def calculate_metrics(data, model):
    """
    The function "calculate_metrics" calculates various clustering metrics for a given dataset and
    model.
    :param data: The data parameter represents the input data that will be used for clustering. It could
    be a numpy array or a pandas DataFrame
    :param model: The "model" parameter refers to the clustering model that you are using to cluster the
    data. It could be any clustering algorithm such as K-means, DBSCAN, or hierarchical clustering. The
    model is used to predict the labels for the data points
    :return: four metrics: inertia, silhouette score, Calinski-Harabasz score, and Davies-Bouldin score.
    """
    labels = model.predict(data)
    inertia = calculate_inertia(data, model)
    silhouette = silhouette_score(data, labels)
    calinski_harabasz = calinski_harabasz_score(data, labels)
    davies_bouldin = davies_bouldin_score(data, labels)
    return inertia, silhouette, calinski_harabasz, davies_bouldin

def display_metrics(train_metrics, validation_metrics):
    """
    The `display_metrics` function creates a GUI window using Tkinter to display clustering metrics for
    both train and validation data.
    :param train_metrics: A list of metrics calculated on the training data. The metrics should be in
    the following order: Inertia (WCSS), Silhouette Score, Calinski-Harabasz Index, Davies-Bouldin Index
    :param validation_metrics: The `validation_metrics` parameter is a list of metrics calculated on the
    validation data. These metrics include "Inertia (WCSS)", "Silhouette Score", "Calinski-Harabasz
    Index", and "Davies-Bouldin Index". Each metric is represented as a floating-point number
    """
    root = tk.Tk()
    root.title("Clustering Metrics")

    metrics = ["Inertia (WCSS)", "Silhouette Score", "Calinski-Harabasz Index", "Davies-Bouldin Index"]

    train_frame = ttk.LabelFrame(root, text="Train Data Metrics", padding="10")
    train_frame.grid(row=0, column=0, padx=20, pady=20, sticky="nsew")
    for i, metric in enumerate(metrics):
        label = ttk.Label(train_frame, text=f"{metric}: {train_metrics[i]:.4f}")
        label.pack(pady=5)

    validation_frame = ttk.LabelFrame(root, text="Validation Data Metrics", padding="10")
    validation_frame.grid(row=0, column=1, padx=20, pady=20, sticky="nsew")
    for i, metric in enumerate(metrics):
        label = ttk.Label(validation_frame, text=f"{metric}: {validation_metrics[i]:.4f}")
        label.pack(pady=5)

    root.mainloop()

def calculate_effectiveness_percentage(data, model):
    """
    The function calculates the effectiveness percentage of a model by comparing the distances between
    data points and their assigned centroids.
    :param data: The data parameter is a numpy array that represents the dataset on which the model is
    being evaluated. It contains the input features for each data point
    :param model: The "model" parameter refers to a clustering model that has been trained on some data.
    It should have a "centroids" attribute that represents the centroids of the clusters in the model
    :return: the effectiveness percentage of the model on the given data.
    """
    centroids = model.centroids
    clusters = model.predict(data)
    distances = np.linalg.norm(data - centroids[clusters], axis=1)
    threshold_distance = np.percentile(distances, 95)
    effectiveness = np.sum(distances < threshold_distance) / len(data)
    return effectiveness

def display_effectiveness(train_effectiveness, validation_effectiveness):
    """
    The function `display_effectiveness` prints the effectiveness percentages of training and validation
    data.
    :param train_effectiveness: The effectiveness of the model on the training data, represented as a
    decimal value between 0 and 1
    :param validation_effectiveness: The parameter `validation_effectiveness` represents the
    effectiveness percentage of the model on the validation data
    """
    print("Effectiveness Percentages:")
    print(f"Training Data: {train_effectiveness * 100:.2f}%")
    print(f"Validation Data: {validation_effectiveness * 100:.2f}%")

def main():
    """
    The main function loads a Spotify dataset, splits it into train, validation, and test sets, performs
    feature scaling and extraction, calculates metrics and effectiveness, and executes a KMeans GUI.
    """
    # Loading the dataset
    file_path = filedialog.askopenfilename(title="Select the Spotify dataset", filetypes=[("CSV files", "*.csv")])
    if not file_path:
        print("No file selected. Exiting...")
        exit()

    spotify_data = pd.read_csv(file_path, encoding="ISO-8859-1")

    # Splitting the data into train, validation, and test sets
    train_data, temp_data = np.split(spotify_data.sample(frac=1, random_state=42), [int(0.7*len(spotify_data))])
    validation_data, test_data = np.split(temp_data, [int(0.5*len(temp_data))])

    # Feature scaling and extraction
    selected_features = ['bpm', 'danceability_%', 'valence_%', 'energy_%', 'acousticness_%', 
                        'instrumentalness_%', 'liveness_%', 'speechiness_%']
    X_train = train_data[selected_features].values
    X_train = (X_train - X_train.mean(axis=0)) / X_train.std(axis=0)

    X_validation = validation_data[selected_features].values
    X_validation = (X_validation - X_train.mean(axis=0)) / X_train.std(axis=0)

    # Calculate metrics
    kmeans_model = KMeans(k=3, max_iters=100, random_seed=42)
    kmeans_model.fit(X_train)
    train_metrics = calculate_metrics(X_train, kmeans_model)
    validation_metrics = calculate_metrics(X_validation, kmeans_model)

    # Display metrics
    display_metrics(train_metrics, validation_metrics)

    # Calculate and display effectiveness
    train_effectiveness = calculate_effectiveness_percentage(X_train, kmeans_model)
    validation_effectiveness = calculate_effectiveness_percentage(X_validation, kmeans_model)
    display_effectiveness(train_effectiveness, validation_effectiveness)

    # Execute KMeans GUI
    app = KMeansGUI(spotify_data, X_train)
    app.mainloop()

# The above code is checking if the current module is being run as the main program. If it is, then it
# calls the `main()` function.
if __name__ == "__main__":
    main()
